# -*- coding: utf-8 -*-
"""nlp_8_nlg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTdTI4ekD7mQX1BIriNmSpbAvc0yj7AM

# Szöveggenerálás

A notebook runtime típusa GPU legyen!
"""

!nvidia-smi

"""## Magyar nyelvű LLM generárotok

https://juniper.nytud.hu/demo/puli

https://huggingface.co/NYTK

## Szöveggenerálás

Vegyük példaként a Huggingface-ről elérhető, előtanított GPT-2 modellt, töltsük be a hozzá tartozó tokenizálót is.

Célszerű az alábbiakhoz GPU-s runtime-ra váltani
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

tokenizer.pad_token = tokenizer.eos_token

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = model.to(device)

"""A tokenizáló működése:"""

input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

print(input_ids, "\n")

for id in input_ids[0]:
  print(id, tokenizer.decode(id, skip_special_tokens=True))

"""A generálás folyamata, és egy lépése:"""

output = model.generate(input_ids,
                        max_length=50,
                        num_return_sequences=1,
                        pad_token_id=tokenizer.pad_token_id)


generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

input_text = "The spiderman was"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

import torch.nn.functional as F

with torch.no_grad():
    outputs = model(input_ids)

# logit-ok az utolsó kimeneti tokenre
last_token_logits = outputs.logits[0, -1, :]

# alkalmazzunk softmax-et, hogy valószínűségi értékeket kapjunk
probs = F.softmax(last_token_logits, dim=-1)

# az 5 legmagasabb valószínűségi értékkel rendelkező token
top_k_probs, top_k_indices = torch.topk(probs, 5)

top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices]

for i, (token, prob) in enumerate(zip(top_k_tokens, top_k_probs)):
    print(f"Top {i+1} token: '{token}', probability:", round(prob.item(), 3), "-->", input_text + f"{token}")

input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

output = model.generate(input_ids,
                        max_length=50,
                        num_return_sequences=1,
                        pad_token_id=tokenizer.pad_token_id,
                        do_sample=True,
                        top_k=0,
                        temperature=1.0)


generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

"""## Generatív Fine-tuning

Szeretnénk a fenti modellt finomhangolni arra, hogy "rossz online értékeléseket" írjon. Ehhez egy online szöveges értékeléseket tartalmazó adatbázisból használjuk az 1 és 2 csillagos review-kat.
"""

!pip install datasets

import pandas as pd
from datasets import Dataset

df = pd.read_parquet("hf://datasets/Yelp/yelp_review_full/yelp_review_full/train-00000-of-00001.parquet")
df

df = df[df.label < 2]
df = df[["text"]].iloc[:10_000]

dataset = Dataset.from_pandas(df)
dataset = dataset.remove_columns("__index_level_0__")

def tokenize_function(examples):
    encoding = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)
    encoding['labels'] = encoding['input_ids'].copy()
    return encoding

tokenized_datasets = dataset.map(tokenize_function, batched=True)

tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""Az alábbi lépés, a finomhangolás folyamata GPU-n körülbelül 5 percet vesz igénybe."""

import os
from transformers import Trainer, TrainingArguments

os.environ["WANDB_DISABLED"] = "true"

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=16,
    logging_dir='./logs',
    logging_steps=50,
    report_to=[],
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
)

trainer.train()

trainer.save_model('./results')

"""Nézzük meg, hogy az eredeti és a finomhangolt modell mit generál, ha "The restaurant" szavakkal promptoljuk be őket."""

input_text = "The restaurant"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

model = AutoModelForCausalLM.from_pretrained('gpt2')
model = model.to(device)

pre_trained_output = model.generate(input_ids, max_length=25, pad_token_id=tokenizer.pad_token_id)
pre_trained_text = tokenizer.decode(pre_trained_output[0], skip_special_tokens=True)
print("Pre-trained model output:")
print(pre_trained_text)



fine_tuned_model = AutoModelForCausalLM.from_pretrained('./results').to(device)

fine_tuned_output = fine_tuned_model.generate(input_ids, max_length=50, pad_token_id=tokenizer.pad_token_id, do_sample=True, temperature=0.7 )
fine_tuned_text = tokenizer.decode(fine_tuned_output[0], skip_special_tokens=True)
print("\nFine-tuned model output:")
print(fine_tuned_text)
#

"""# In-context learning

Az in-context "few-shot learning" azt jelenti, hogy egy szöveges propmt-ban néhány példát feltüntetve a modell képes általánosítani az új, még nem látott példákra.

## Szentiment osztályozási feladat
"""

import pandas as pd

df = pd.read_parquet("hf://datasets/Yelp/yelp_review_full/yelp_review_full/train-00000-of-00001.parquet")

neg = df[df.label == 0][:1000]
pos = df[df.label == 4][:1000]
df = pd.concat([neg, pos])
df

train_data = df.sample(frac = 0.9)
test_data = df.drop(train_data.index)
train_data.reset_index(drop=True,inplace=True)
test_data.reset_index(drop=True,inplace=True)

from sklearn.metrics import accuracy_score
from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy="most_frequent") # tanító adatbázis leggyakoribb osztálya lesz mindig a predikció
dummy_clf.fit(train_data.text, train_data.label)
baseline_prediction = dummy_clf.predict(test_data) # predikció a kiértékelő adatbázison
accuracy_score(baseline_prediction, test_data.label)

"""## Zero-shot predikció

Az eddigi GPT2-es modell helyett használjuk a Llama 3.2 1 milliárd paraméteres változatát. Ennek a modellnek előnye, hogy hosszabb szövegekkel is megbírkózik, viszont sokkal tovább tartott volna fine-tuneing rajta.
"""

tokenizer = AutoTokenizer.from_pretrained("unsloth/Llama-3.2-1B-Instruct")
model = AutoModelForCausalLM.from_pretrained("unsloth/Llama-3.2-1B-Instruct")

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = model.to(device)

"""A prompt készítéséhez a numerikus címkéket érdemes szövegessé alakítani:

"""

def get_answer(label):
    if label==0:
        return 'It is negative. '
    else:
        return 'It is positive. '

"""Az előző művelet inverze. A szöveges címkét képezi binárissá, mely a calculate_accuracy függvényhez kell."""

def get_prediction(text):
    if 'negative' in text:
        return 0
    elif 'positive' in text:
        return 4
    else:
         return -1

import random

def get_prompt(train_data, val_sentence):
  prompt = val_sentence['text'] + '\n Is this review positive or negative? I think it is '
  return prompt, len(prompt)

"""A promptolás során végigiterálunk a test_data validációs adatkészleten úgy, hogy minden adathoz megadunk egy few-shot példákból álló promptot.
1. Előállítjuk a promptot
2. A tokenizer segítségével megkapjuk az `input_id`-kat, a modell bemeneteit
3. melyet átadunk a model.generate metódusnak
  - **`do_sample`**: szöveggenerálás során használjon-e mintavételezést
  - **`max_length`**: a generált szöveg maximális hossza (szavakban)
  - **`temperature`**: szabályozza a szöveggenerálásban a véletlenség szintjét
  - **`top_k`**: szabályozza, hány legvalószínűbb token legyen figyelembe véve minden generálási lépésnél
  - **`top_p`**: ha < 1 értékre van állítva, akkor a rendszer csak a legvalószínűbb tokenek legkisebb halmazát tartja meg a generálás során, ha azok valószínűsége eléri a `top_p` vagy nagyobb valószínűséget
([lásd bővebben](https://huggingface.co/blog/how-to-generate) )

4. A kimenetben kapott `generated_id`-ket dekódolni kell, hogy visszakapjuk a generált szót
5. Kinyerjük az eredményt, s elmentjük a predikált és a valós címkéket
"""

def eval(train_data, test_data):
  predictions = []
  labels = []

  for i in range(len(test_data)):
    prompt, prompt_length = get_prompt(train_data, test_data.iloc[i])
    input_ids = tokenizer(prompt, return_tensors='pt', max_length=1000).input_ids.to(model.device)
    ml = int(input_ids.size()[1])
    with torch.no_grad():
      generated_ids = model.generate(input_ids, do_sample=True, max_length=ml + 2, temperature=0.3, top_k=10, top_p=0.1, pad_token_id=tokenizer.pad_token_id)
    print('-----------------------------------------------------------------------------------------------------')
    generated_text = tokenizer.decode(generated_ids[0])
    print(generated_text)
    predictions.append(get_prediction(generated_text[-10:]))
    labels.append(test_data.iloc[i]['label'])
    print('predicted: '+str(predictions[i])+' label: '+str(labels[i]))
  return predictions, labels

#tokenizer = AutoTokenizer.from_pretrained('gpt2', truncation_side="left")

labels, predictions = eval([], test_data)

accuracy = accuracy_score(labels, predictions)
print(f'Accuracy: {accuracy:.2f}')

"""## Few-shot learning

Prompt képzés. Egyenlő arányban vesz negatív és pozitív példákat a train set-ből, majd hozzá konkatenálja a validation set-ből a soron következő elemet címke nélkül.
Példa:



```
Few-shot examples:
This is a great movie with outstanding acting.
Is this sentence positive or negative?
Answer: positive

This is bad, a complete a waste of time.
Is this sentence positive or negative?
Answer: negative

Prompt:
I really enjoyed this movie, the set was spectacular.
Is this sentence positive or negative?
Answer:

```



"""

import random

def get_prompt(train_data, val_sentence):
  limit = 2
  labels = [0, 4]
  few_shot_data = []


  for label in labels:
    c=0
    while c<limit:
      sample = train_data.sample(1).iloc[0]
      if sample["label"] == label:
        few_shot_data.append(sample)
        c+=1
  random.shuffle(few_shot_data)
  fin_few_shot_string = ''

  for j in range(0,len(few_shot_data)):
    fin_few_shot_string += few_shot_data[j]['text'] + ' Is this review positive or negative? '
    fin_few_shot_string += get_answer(few_shot_data[j]['label'])
    fin_few_shot_string += "\n"
  fin_few_shot_string += val_sentence['text'] + ' Is this review positive or negative?  I think it is '

  return fin_few_shot_string, len(fin_few_shot_string)

prompt, prompt_length = get_prompt(train_data, test_data.iloc[0,:])
print (prompt)

input_ids = tokenizer(prompt, return_tensors='pt', max_length=1000).input_ids.to(model.device)
ml = int(input_ids.size()[1])
with torch.no_grad():
  generated_ids = model.generate(input_ids, do_sample=True, max_length=ml + 2, temperature=0.3, top_k=10, top_p=0.1)
generated_text = tokenizer.decode(generated_ids[0])
print(generated_text)

labels, predictions = eval(train_data, test_data)

accuracy = accuracy_score(labels, predictions)
print(f'Accuracy: {accuracy:.2f}')

"""Kérdés: Hogyan lehetne javítani az eredményen? Lehet-e?

# Gyakorló feladatok



*   Promptolással lehet javítani a zero-shot eredményeken?
*   Fine-tuneoljuk a GPT2-t a sentiment osztályozási feladatra! Milyen eredményeket ér el?
"""

